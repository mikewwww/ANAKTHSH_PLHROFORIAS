{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bbfa135-1483-47b5-bfce-f84f9ed43884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Προστέθηκαν τα ακόλουθα άρθρα:\n",
      "Python (programming language)\n",
      "Downloading Python (programming language) ...\n",
      "Crawl 50 :: Python (programming language)\n",
      "Add article 2/50 \"Hello, World!\" program\n",
      "Add article 3/50 3ds Max\n",
      "Add article 4/50 ?:\n",
      "Add article 5/50 ABC (programming language)\n",
      "Add article 6/50 ADMB\n",
      "Add article 7/50 ALGOL\n",
      "Add article 8/50 ALGOL 68\n",
      "Add article 9/50 API\n",
      "Add article 10/50 APL (programming language)\n",
      "Add article 11/50 ATmega\n",
      "Add article 12/50 AVR microcontrollers\n",
      "Add article 13/50 Abaqus\n",
      "Add article 14/50 Academic Free License\n",
      "Add article 15/50 Academic conference\n",
      "Add article 16/50 Ada (programming language)\n",
      "Add article 17/50 Advanced Simulation Library\n",
      "Add article 18/50 Ahead-of-time compilation\n",
      "Add article 19/50 Alex Martelli\n",
      "Add article 20/50 Algebra\n",
      "Add article 21/50 Alternative terms for free software\n",
      "Add article 22/50 Amazon (company)\n",
      "Add article 23/50 AmigaOS 4\n",
      "Add article 24/50 Amoeba (operating system)\n",
      "Add article 25/50 Anaconda (installer)\n",
      "Add article 26/50 Analyse-it\n",
      "Add article 27/50 Android (operating system)\n",
      "Add article 28/50 Anonymous function\n",
      "Add article 29/50 Apache Groovy\n",
      "Add article 30/50 Apache License\n",
      "Add article 31/50 Apache webserver\n",
      "Add article 32/50 Aphorism\n",
      "Add article 33/50 Apple M1\n",
      "Add article 34/50 Apple Public Source License\n",
      "Add article 35/50 ArXiv (identifier)\n",
      "Add article 36/50 Arbitrary-precision arithmetic\n",
      "Add article 37/50 ArcGIS\n",
      "Add article 38/50 Arithmetic operations\n",
      "Add article 39/50 Array index\n",
      "Add article 40/50 Array slicing\n",
      "Add article 41/50 Artificial intelligence\n",
      "Add article 42/50 Artistic License\n",
      "Add article 43/50 Aspect-oriented programming\n",
      "Add article 44/50 Assembly language\n",
      "Add article 45/50 Assertion (programming)\n",
      "Add article 46/50 Assignment (computer science)\n",
      "Add article 47/50 Associative array\n",
      "Add article 48/50 Astropy\n",
      "Add article 49/50 Asynchronous Server Gateway Interface\n",
      "Downloading Arbitrary-precision arithmetic ...\n",
      "Downloading APL (programming language) ...\n",
      "Downloading ArXiv ...\n",
      "Downloading AVR microcontrollers ...\n",
      "Downloading Ada (programming language) ...\n",
      "Downloading Anonymous function ...\n",
      "Downloading ABC (programming language) ...\n",
      "Downloading Associative array ...\n",
      "Downloading Aphorism ...\n",
      "Downloading Artistic License ...\n",
      "Downloading Assignment (computer science) ...\n",
      "Downloading Autodesk 3ds Max ...\n",
      "Downloading Apple Public Source License ...\n",
      "Downloading Array slicing ...\n",
      "Downloading Amoeba (operating system) ...\n",
      "Downloading Advanced Simulation Library ...\n",
      "Downloading Amazon (company) ...\n",
      "Downloading Python (programming language) ...\n",
      "Downloading Artificial intelligence ...\n",
      "Downloading Ahead-of-time compilation ...\n",
      "Downloading Analyse-it ...\n",
      "Downloading AmigaOS 4 ...\n",
      "Downloading Android (operating system) ...\n",
      "Downloading Assembly language ...\n",
      "Downloading AVR microcontrollers ...\n",
      "Downloading ArcGIS ...\n",
      "Downloading Abaqus ...\n",
      "Downloading Ternary conditional operator ...\n",
      "Downloading Anaconda (installer) ...\n",
      "Downloading Academic conference ...\n",
      "Downloading Apache License ...\n",
      "Downloading ALGOL ...\n",
      "Downloading Array (data structure) ...\n",
      "Downloading API ...\n",
      "Downloading Aspect-oriented programming ...\n",
      "Downloading Asynchronous Server Gateway Interface ...\n",
      "Downloading Alex Martelli ...\n",
      "Downloading Algebra ...\n",
      "Downloading Apple M1 ...\n",
      "Downloading Python (programming language) ...\n",
      "Downloading Assertion (software development) ...\n",
      "Downloading Academic Free License ...\n",
      "Downloading Apache Groovy ...\n",
      "Downloading Arithmetic ...\n",
      "Downloading Alternative terms for free software ...\n",
      "Downloading Apache HTTP Server ...\n",
      "Downloading ADMB ...\n",
      "Downloading Astropy ...\n",
      "Downloading ALGOL 68 ...\n",
      "Downloading \"Hello, World!\" program ...\n",
      "Κατηγορίες του άρθρου 'Python (programming language)': {'class-based programming languages': 1, 'computer science in the netherlands': 1, 'concurrent programming languages': 1, 'cross-platform free software': 1, 'cross-platform software': 1, 'dutch inventions': 1, 'dynamically typed programming languages': 1, 'educational programming languages': 1, 'high-level programming languages': 1, 'information technology in the netherlands': 1, 'multi-paradigm programming languages': 1, 'notebook interface': 1, 'object-oriented programming languages': 1, 'pattern matching programming languages': 1, 'programming languages': 1, 'programming languages created in 1991': 1, 'python (programming language)': 1, 'scripting languages': 1, 'text-oriented programming languages': 1, 'use american english from december 2024': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Στατιστικά ανάλυσης για το σύνολο κειμένων:\n",
      "{'files': 48, 'paras': 2352, 'sents': 8246, 'words': 180393, 'vocab': 12650, 'time': 0.9748818874359131}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Δώσε ερώτημα για αναζήτηση:  machine AND python\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Αποτελέσματα TF-IDF αναζήτησης:\n",
      "Python_(programming_language).txt: 0.022585888285353548\n",
      "Alex_Martelli.txt: 0.016612734080165125\n",
      "Astropy.txt: 0.015065804854245716\n",
      "Asynchronous_Server_Gateway_Interface.txt: 0.008588017062008653\n",
      "Ahead-of-time_compilation.txt: 0.0058084959264799885\n",
      "Assembly_language.txt: 0.003190662150715329\n",
      "Anonymous_function.txt: 0.0027288827706926605\n",
      "Amoeba_(operating_system).txt: 0.0020665921207268013\n",
      "Artificial_intelligence.txt: 0.001981564740445601\n",
      "Array_slicing.txt: 0.001718941109763414\n",
      "Arbitrary-precision_arithmetic.txt: 0.00166657782537821\n",
      "Anaconda_(installer).txt: 0.0016063467139273825\n",
      "Ternary_conditional_operator.txt: 0.0007562775441804495\n",
      "Apache_Groovy.txt: 0.0007212791395955005\n",
      "Abaqus.txt: 0.0007053756423573742\n",
      "Aspect-oriented_programming.txt: 0.0006838547070894721\n",
      "APL_(programming_language).txt: 0.0006367911626641666\n",
      "Assertion_(software_development).txt: 0.0006285406193752595\n",
      "ADMB.txt: 0.0005549617138190114\n",
      "Assignment_(computer_science).txt: 0.0005402094425477075\n",
      "Array_(data_structure).txt: 0.0004637987156640651\n",
      "ArcGIS.txt: 0.00039934431960276314\n",
      "AmigaOS_4.txt: 0.0003554139106177494\n",
      "Associative_array.txt: 0.00032814754459355096\n",
      "API.txt: 0.00020143771594302392\n",
      "ALGOL_68.txt: 0.0001711897210570376\n",
      "Android_(operating_system).txt: 0.00013365738151946497\n",
      "AVR_microcontrollers.txt: 0.0001095711635409335\n",
      "Algebra.txt: 0.00010350114686575262\n",
      "Apple_M1.txt: 0.0\n",
      "ABC_(programming_language).txt: 0.0\n",
      "Apache_HTTP_Server.txt: 0.0\n",
      "Hello,_World_program.txt: 0.0\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Δώσε Αλγόριθμο ανάκτησης(Boolean, VSM, BM25):  Boolean\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Αποτελέσματα Boolean αναζήτησης:\n",
      "['Astropy.txt', 'Array_slicing.txt', 'Aspect-oriented_programming.txt', 'Abaqus.txt', 'ABC_(programming_language).txt', 'Anaconda_(installer).txt', 'Apache_Groovy.txt', 'Assertion_(software_development).txt', 'Python_(programming_language).txt', 'Ternary_conditional_operator.txt', 'Arbitrary-precision_arithmetic.txt', 'Apache_HTTP_Server.txt', 'ArcGIS.txt', 'Asynchronous_Server_Gateway_Interface.txt', 'Assignment_(computer_science).txt', 'Associative_array.txt', 'Alex_Martelli.txt', 'AmigaOS_4.txt', 'Artificial_intelligence.txt', 'Hello,_World_program.txt', 'Anonymous_function.txt', 'Amoeba_(operating_system).txt']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import json\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "import wikipediaapi as wiki_api\n",
    "\n",
    "# Δημιουργία κλάσης WikipediaReader\n",
    "class WikipediaReader():\n",
    "    def __init__(self, dir=\"articles\"):\n",
    "        # Αρχικοποίηση της κλάσης, ορίζουμε τον φάκελο αποθήκευσης άρθρων και τον πυρήνα Wikipedia API.\n",
    "        self.pages = set()\n",
    "        self.article_path = os.path.join(\"./\", dir)\n",
    "        self.wiki = wiki_api.Wikipedia(\n",
    "            language='en',  # Ορίζουμε τη γλώσσα Wikipedia σε Αγγλικά.\n",
    "            extract_format=wiki_api.ExtractFormat.WIKI,  # Ορίζουμε τη μορφή εξαγωγής σε κείμενο Wiki.\n",
    "            user_agent=\"MyWikipediaReaderApp/1.0 (myemail@example.com)\"  # Πρόσθεσε user_agent\n",
    "        )\n",
    "        # Δημιουργία φακέλου αν δεν υπάρχει\n",
    "        try:\n",
    "            os.mkdir(self.article_path)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "    # Μέθοδος για να μετατρέπουμε τον τίτλο του άρθρου σε μορφή συμβατή με αρχεία (π.χ., αντικατάσταση κενών).\n",
    "    def _get_page_title(self, article):\n",
    "        return re.sub(r'\\s+', '_', article)\n",
    "\n",
    "    # Μέθοδος για προσθήκη ενός άρθρου\n",
    "    def add_article(self, article):\n",
    "        try:\n",
    "            # Παίρνουμε τη σελίδα από τη Wikipedia με τον δεδομένο τίτλο\n",
    "            page = self.wiki.page(self._get_page_title(article))\n",
    "            # Ελέγχουμε αν η σελίδα υπάρχει\n",
    "            if page.exists():\n",
    "                self.pages.add(page)  # Προσθέτουμε τη σελίδα στο σύνολο `pages`\n",
    "                return page\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Μέθοδος που επιστρέφει τη λίστα των σελίδων που έχουν προστεθεί\n",
    "    def list(self):\n",
    "        return self.pages\n",
    "\n",
    "    # Μέθοδος για λήψη και αποθήκευση των άρθρων σε αρχεία\n",
    "    def process(self, update=False):\n",
    "        for page in self.pages:\n",
    "            # Δημιουργία έγκυρου ονόματος αρχείου για την αποθήκευση\n",
    "            filename = re.sub('\\s+', '_', f'{page.title}')\n",
    "            filename = re.sub(r'[\\\\/*?:\"<>|!]', '', filename)\n",
    "            file_path = os.path.join(self.article_path, f'{filename}.txt')\n",
    "            # Αν επιτρέπεται το update ή το αρχείο δεν υπάρχει, κατεβάζουμε και αποθηκεύουμε το άρθρο\n",
    "            if update or not os.path.exists(file_path):\n",
    "                print(f'Downloading {page.title} ...')\n",
    "                content = page.text\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "            else:\n",
    "                print(f'Not updating {page.title} ...')\n",
    "\n",
    "    # Μέθοδος που κάνει \"crawl\" τα συνδεδεμένα άρθρα με δεδομένο βάθος και μέγιστο αριθμό άρθρων\n",
    "    def crawl_pages(self, article, depth=3, total_number=1000):\n",
    "        print(f'Crawl {total_number} :: {article}')\n",
    "        page = self.add_article(article)  # Προσθέτουμε το αρχικό άρθρο στη λίστα\n",
    "        childs = set()  # Δημιουργούμε σύνολο για τα συνδεδεμένα άρθρα\n",
    "\n",
    "        if page:\n",
    "            for child in page.links.keys():\n",
    "                # Αν δεν έχουμε φτάσει το όριο άρθρων, προσθέτουμε συνδεδεμένο άρθρο\n",
    "                if len(self.pages) < total_number:\n",
    "                    print(f'Add article {len(self.pages)}/{total_number} {child}')\n",
    "                    self.add_article(child)\n",
    "                    childs.add(child)\n",
    "\n",
    "        # Μειώνουμε το βάθος και συνεχίζουμε το crawl για κάθε συνδεδεμένο άρθρο\n",
    "        depth -= 1\n",
    "        if depth > 0:\n",
    "            for child in sorted(childs):\n",
    "                if len(self.pages) < total_number:\n",
    "                    self.crawl_pages(child, depth, len(self.pages))\n",
    "\n",
    "    # Μέθοδος για λήψη των κατηγοριών του άρθρου, με φιλτράρισμα περιττών κατηγοριών\n",
    "    def get_categories(self, title):\n",
    "        page = self.add_article(title)\n",
    "        if page:\n",
    "            # Φιλτράρισμα μη σχετικών κατηγοριών\n",
    "            if (list(page.categories.keys())) and (len(list(page.categories.keys())) > 0):\n",
    "                categories = [c.replace('Category:', '').lower() for c in list(page.categories.keys())\n",
    "                              if c.lower().find('articles') == -1\n",
    "                              and c.lower().find('pages') == -1\n",
    "                              and c.lower().find('wikipedia') == -1\n",
    "                              and c.lower().find('cs1') == -1\n",
    "                              and c.lower().find('webarchive') == -1\n",
    "                              and c.lower().find('dmy dates') == -1\n",
    "                              and c.lower().find('short description') == -1\n",
    "                              and c.lower().find('commons category') == -1]\n",
    "                # Επιστρέφει λεξικό με κάθε κατηγορία και την τιμή 1\n",
    "                return dict.fromkeys(categories, 1)\n",
    "        return {}\n",
    "    \n",
    "    ############################################################################################################\n",
    "    \n",
    "    # Εισαγωγή και δημιουργία ενός αντικειμένου WikipediaReader\n",
    "reader = WikipediaReader(dir=\"my_articles\")\n",
    "\n",
    "# 1. Προσθήκη ενός άρθρου, π.χ., \"Python (programming language)\"\n",
    "article = reader.add_article(\"Python (programming language)\")\n",
    "\n",
    "# 2. Καταγραφή όλων των άρθρων που έχουν προστεθεί\n",
    "print(\"Προστέθηκαν τα ακόλουθα άρθρα:\")\n",
    "for page in reader.list():\n",
    "    print(page.title)\n",
    "\n",
    "# 3. Λήψη και αποθήκευση των άρθρων σε αρχεία\n",
    "reader.process(update=False)  # Αν το αρχείο υπάρχει ήδη, δεν θα το ενημερώσει\n",
    "\n",
    "# 4. Κάνοντας crawl σε συνδεδεμένα άρθρα για την επέκταση της βάσης δεδομένων\n",
    "# Κάνουμε crawl στο άρθρο \"Python (programming language)\" μέχρι βάθος 2 και με όριο 50 άρθρα.\n",
    "reader.crawl_pages(\"Python (programming language)\", depth=2, total_number=50)\n",
    "\n",
    "# 5. Ενημέρωση για όλα τα νέα άρθρα που προστέθηκαν\n",
    "reader.process(update=True)\n",
    "\n",
    "# 6. Ανάκτηση των κατηγοριών για ένα συγκεκριμένο άρθρο, π.χ., \"Python (programming language)\"\n",
    "categories = reader.get_categories(\"Python (programming language)\")\n",
    "print(\"Κατηγορίες του άρθρου 'Python (programming language)':\", categories)\n",
    "\n",
    "\n",
    "# Κατεβάζει τα απαραίτητα δεδομένα αν δεν υπάρχουν\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "class WikipediaCorpus(PlaintextCorpusReader):\n",
    "    def __init__(self, root, fileids):\n",
    "        super().__init__(root, fileids)\n",
    "        self.lemmatizer = WordNetLemmatizer()  # Αρχικοποίηση του lemmatizer\n",
    "        self.stop_words = set(stopwords.words('english'))  # Χρήση αγγλικών stop words\n",
    "        self.inverted_index = self.create_inverted_index()  # Δημιουργία αντεστραμμένου ευρετηρίου\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        # Καθαρισμός κειμένου: αφαιρεί σημεία στίξης, κάνει tokenization,αφαιρεί stop words και εφαρμόζει lemmatization. \n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text).lower()  # Αφαίρεση μη αλφαριθμητικών χαρακτήρων και μετατροπή σε πεζά\n",
    "        tokens = word_tokenize(text)  # Tokenization\n",
    "        tokens = [word for word in tokens if word not in self.stop_words]  # Αφαίρεση stop words\n",
    "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens]  # Εφαρμογή lemmatization\n",
    "        return tokens\n",
    "\n",
    "    def preprocess_documents(self):\n",
    "        # Καθαρισμός όλων των εγγράφων του corpus. \n",
    "        cleaned_docs = {}\n",
    "        for fileid in self.fileids():  # Διατρέχει όλα τα αρχεία του corpus\n",
    "            raw_text = self.raw(fileids=fileid)  # Παίρνει το ακατέργαστο κείμενο\n",
    "            cleaned_docs[fileid] = self.clean_text(raw_text)  # Καθαρισμός κειμένου\n",
    "        return cleaned_docs\n",
    "    \n",
    "    def vocab(self):\n",
    "        # Υπολογισμός συχνότητας λέξεων στο καθαρισμένο σύνολο δεδομένων. \n",
    "        return nltk.FreqDist(word for fileid in self.fileids() for word in self.clean_text(self.raw(fileid)))\n",
    "    \n",
    "    def max_words(self):\n",
    "        # Υπολογισμός μέγιστου αριθμού λέξεων σε έγγραφο.  \n",
    "        return max(len(self.clean_text(self.raw(fileid))) for fileid in self.fileids())\n",
    "    \n",
    "    def create_inverted_index(self):\n",
    "        # Δημιουργία αντεστραμμένου ευρετηρίου. \n",
    "        inverted_index = defaultdict(list)\n",
    "        for fileid in self.fileids():  # Διατρέχει όλα τα αρχεία\n",
    "            tokens = self.clean_text(self.raw(fileids=fileid))  # Καθαρισμός κειμένου\n",
    "            for token in set(tokens):  # Για κάθε μοναδικό token\n",
    "                inverted_index[token].append(fileid)  # Προσθήκη στο ευρετήριο\n",
    "        return inverted_index\n",
    "\n",
    "    def query_processing(self, query):\n",
    "        # Εκτέλεση αναζήτησης. Υποστηρίζει τελεστές AND, OR, NOT. \n",
    "        tokens = re.findall(r'\\w+|AND|OR|NOT', query)  # Διαχωρισμός του ερωτήματος σε tokens\n",
    "        result_docs = set(self.fileids())  # Αρχικοποίηση με όλα τα έγγραφα\n",
    "        current_op = 'OR'  # Προκαθορισμένος τελεστής\n",
    "        exclude_docs = set()\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in {'AND', 'OR', 'NOT'}:  # Αν είναι τελεστής, αποθηκεύεται\n",
    "                current_op = token\n",
    "            else:\n",
    "                token_docs = set(self.inverted_index.get(token, []))  # Ανάκτηση σχετικών εγγράφων για το token\n",
    "                if current_op == 'AND':\n",
    "                    result_docs &= token_docs\n",
    "                elif current_op == 'OR':\n",
    "                    result_docs |= token_docs  \n",
    "                elif current_op == 'NOT':\n",
    "                    exclude_docs |= token_docs  # Προσθήκη στον αποκλεισμό\n",
    "\n",
    "        result_docs -= exclude_docs  # Αποκλεισμός εγγράφων\n",
    "        return list(result_docs)\n",
    "\n",
    "    def compute_tf_idf(self, query):\n",
    "        # Υπολογισμός TF-IDF και επιστροφή εγγράφων με βάση το σκορ συσχέτισης. \n",
    "        N = len(self.fileids())  # Συνολικός αριθμός εγγράφων\n",
    "        query_tokens = self.clean_text(query)  # Καθαρισμός του ερωτήματος\n",
    "        scores = defaultdict(float)\n",
    "\n",
    "        for token in query_tokens:\n",
    "            if token in self.inverted_index:\n",
    "                df = len(self.inverted_index[token])  # Document frequency\n",
    "                idf = math.log(N / (1 + df))  # Υπολογισμός IDF\n",
    "                for doc in self.inverted_index[token]:\n",
    "                    tf = self.raw(fileids=doc).lower().split().count(token) / len(self.raw(fileids=doc).split())\n",
    "                    scores[doc] += tf * idf  # Υπολογισμός TF-IDF\n",
    "\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def vector_space_model(self, query):\n",
    "        # Υλοποίηση Vector Space Model (VSM) με χρήση cosine similarity. \n",
    "        N = len(self.fileids())  # Συνολικός αριθμός εγγράφων\n",
    "        query_tokens = self.clean_text(query)  # Καθαρισμός του ερωτήματος\n",
    "        query_vector = defaultdict(float)\n",
    "        doc_vectors = {doc: defaultdict(float) for doc in self.fileids()}  # Δημιουργία διανυσμάτων για κάθε έγγραφο\n",
    "\n",
    "        # Υπολογισμός TF-IDF για το ερώτημα\n",
    "        for token in query_tokens:\n",
    "            if token in self.inverted_index:\n",
    "                df = len(self.inverted_index[token])  # Συχνότητα εγγράφων για το token\n",
    "                idf = math.log(N / (1 + df))\n",
    "                query_vector[token] = idf  # Προσθήκη στο διάνυσμα του ερωτήματος\n",
    "                for doc in self.inverted_index[token]:\n",
    "                    tf = self.raw(fileids=doc).lower().split().count(token) / len(self.raw(fileids=doc).split())\n",
    "                    doc_vectors[doc][token] = tf * idf  # Υπολογισμός TF-IDF για κάθε έγγραφο\n",
    "\n",
    "        # Υπολογισμός cosine similarity\n",
    "        def cosine_similarity(vec1, vec2):\n",
    "            dot_product = sum(vec1[token] * vec2.get(token, 0) for token in vec1)  # Εσωτερικό γινόμενο\n",
    "            norm1 = math.sqrt(sum(v ** 2 for v in vec1.values()))  # Μέτρο του πρώτου διανύσματος\n",
    "            norm2 = math.sqrt(sum(v ** 2 for v in vec2.values()))  # Μέτρο του δεύτερου διανύσματος\n",
    "            return dot_product / (norm1 * norm2) if norm1 and norm2 else 0  # Υπολογισμός cosine similarity\n",
    "\n",
    "        scores = {doc: cosine_similarity(query_vector, doc_vectors[doc]) for doc in self.fileids()}\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def probabilistic_retrieval(self, query, k1=1.5, b=0.75):\n",
    "        # Υλοποίηση Probabilistic Retrieval Model με χρήση BM25. \n",
    "        N = len(self.fileids())  # Συνολικός αριθμός εγγράφων\n",
    "        avg_doc_length = sum(len(self.clean_text(self.raw(fileids=doc))) for doc in self.fileids()) / N  # Μέσο μήκος εγγράφου\n",
    "        query_tokens = self.clean_text(query)  # Καθαρισμός του ερωτήματος\n",
    "        scores = defaultdict(float)\n",
    "\n",
    "        for token in query_tokens:\n",
    "            if token in self.inverted_index:\n",
    "                df = len(self.inverted_index[token])  # Συχνότητα εγγράφων\n",
    "                idf = math.log((N - df + 0.5) / (df + 0.5) + 1)  # Υπολογισμός IDF\n",
    "                for doc in self.inverted_index[token]:\n",
    "                    doc_length = len(self.clean_text(self.raw(fileids=doc)))  # Μήκος εγγράφου\n",
    "                    tf = self.raw(fileids=doc).lower().split().count(token)  # Συχνότητα λέξης\n",
    "                    score = idf * ((tf * (k1 + 1)) / (tf + k1 * (1 - b + b * (doc_length / avg_doc_length))))  # Υπολογισμός BM25\n",
    "                    scores[doc] += score  # Προσθήκη στο σκορ\n",
    "\n",
    "        return sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    def describe(self, fileids=None):\n",
    "        # Παροχή στατιστικών για το corpus. \n",
    "        started = time()\n",
    "        stats = {\n",
    "            'files': len(self.fileids()),  # Αριθμός αρχείων\n",
    "            'paras': len(self.paras()),  # Αριθμός παραγράφων\n",
    "            'sents': len(self.sents()),  # Αριθμός προτάσεων\n",
    "            'words': len(self.words()),  # Αριθμός λέξεων\n",
    "            'vocab': len(self.vocab()),  # Μέγεθος λεξιλογίου\n",
    "            'time': time() - started  # Χρόνος επεξεργασίας\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "\n",
    "# Εφαρμογή του corpus και προεπεξεργασία\n",
    "corpus = WikipediaCorpus('my_articles', '.*\\.txt')\n",
    "\n",
    "# Περιγραφή του αρχικού συνόλου δεδομένων\n",
    "stats = corpus.describe()\n",
    "print(\"\\nΣτατιστικά ανάλυσης για το σύνολο κειμένων:\")\n",
    "print(stats)\n",
    "query = input(\"\\nΔώσε ερώτημα για αναζήτηση: \")\n",
    "\n",
    "# Εκτέλεση αναζήτησης με TF-IDF\n",
    "tf_idf_results = corpus.compute_tf_idf(query)\n",
    "print(\"\\nΑποτελέσματα TF-IDF αναζήτησης:\")\n",
    "for doc, score in tf_idf_results:\n",
    "    print(f\"{doc}: {score}\")\n",
    "    \n",
    "algo = input(\"\\nΔώσε Αλγόριθμο ανάκτησης(Boolean, VSM, BM25): \")\n",
    "\n",
    "if algo == 'Boolean':\n",
    "    # Εκτέλεση ερωτήματος Boolean\n",
    "    result_docs = corpus.query_processing(query)\n",
    "    print(\"\\nΑποτελέσματα Boolean αναζήτησης:\")\n",
    "    print(result_docs)\n",
    "elif algo == 'VSM':\n",
    "    # Εκτέλεση αναζήτησης με Vector Space Model\n",
    "    vsm_results = corpus.vector_space_model(query)\n",
    "    print(\"\\nΑποτελέσματα Vector Space Model αναζήτησης:\")\n",
    "    for doc, score in vsm_results:\n",
    "        print(f\"{doc}: {score}\")\n",
    "elif algo == 'BM25':\n",
    "    # Εκτέλεση αναζήτησης με Probabilistic Retrieval Model (BM25)\n",
    "    bm25_results = corpus.probabilistic_retrieval(query)\n",
    "    print(\"\\nΑποτελέσματα Probabilistic Retrieval Model (BM25) αναζήτησης:\")\n",
    "    for doc, score in bm25_results:\n",
    "        print(f\"{doc}: {score}\")\n",
    "else:\n",
    "    print(\"Λάθος αλγόριθμος ανάκτησης!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2615c62-920e-4683-9aec-dc8b435ae819",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
